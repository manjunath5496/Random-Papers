# Random Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(1).pdf" style="text-decoration:none;">Training Deeper Neural Machine Translation Models
with Transparent Attention</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(2).pdf" style="text-decoration:none;">Exploiting Deep Representations for Neural Machine Translation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(3).pdf" style="text-decoration:none;">Fixup Initialization:
Residual Learning Without Normalization</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(4).pdf" style="text-decoration:none;">Are Sixteen Heads Really Better than One?</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(5).pdf" style="text-decoration:none;">Depth Growing for Neural Machine Translation</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(6).pdf" style="text-decoration:none;">A Closer Look At Deep Learning Heuristics:
Learning Rate Restarts, Warmup And Distillation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(7).pdf" style="text-decoration:none;">Adaptive Gradient Methods With Dynamic
Bound Of Learning Rate</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(8).pdf" style="text-decoration:none;"> Efficient Training of BERT by Progressively Stacking </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(9).pdf" style="text-decoration:none;">Towards a Deep and Unified Understanding of Deep Neural Models in NLP</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(10).pdf" style="text-decoration:none;">Layer-Wise Coordination between Encoder and
Decoder for Neural Machine Translation </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Random-Papers/blob/master/r(11).pdf" style="text-decoration:none;">Multi-layer Representation Fusion for Neural Machine Translation</a></li>
</ul>
